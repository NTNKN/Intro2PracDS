{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実践データ科学入門 2020年度木曜4限\n",
    "\n",
    "# 第3回 その1 重線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook # if necessary to rotate figures in 3D plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import art3d\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重線形回帰とは\n",
    "\n",
    "重回帰は目的変数に対して説明変数が複数ある場合に用いる手法で，\n",
    "\n",
    "$$\n",
    "y = a_0 + a_1 x_1 + a_2 x_2 + \\ldots + a_M x_M = a_0 + \\sum_{j=1}^M a_j x_j + \\xi\n",
    "$$\n",
    "\n",
    "というモデルを当てはめる方法である．ただし，\n",
    "\n",
    "- $y$ は目的変数（データとして与えられるもの）\n",
    "- $x_j \\ (j=1,2,\\ldots,M)$ は説明変数（データとして与えられるもの）\n",
    "- $a_j \\ (j=0,1,2,\\ldots,M)$ は回帰係数（データから求めるもの）\n",
    "- $\\xi$ はノイズ（モデルでは当てはめられないランダムな要因）\n",
    "\n",
    "以下，特に注意しない限り__データは実数値__とする．\n",
    "\n",
    "\n",
    "$M$ 個の説明変数と $M$ 個の回帰係数を \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\mathbf X = (x_1, x_2, \\ldots, x_M) \\in \\mathbb R^M,\\\\\n",
    "& \\mathbf A = (a_1, a_2, \\ldots, a_M) \\in \\mathbb R^M\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "とそれぞれ $M$ 次元ベクトルで表すことで\n",
    "\n",
    "$$\n",
    "y = a_0 + \\mathbf X \\cdot \\mathbf A + \\xi\n",
    "$$\n",
    "\n",
    "と内積の形で表示することができる．さらに定数の部分もまとめて \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\tilde{\\mathbf X} = (1, x_1, x_2, \\ldots, x_M) \\in \\mathbb R^M,\\\\\n",
    "& \\tilde{\\mathbf A} = (a_0, a_1, a_2, \\ldots, a_M) \\in \\mathbb R^M\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "とおくことで\n",
    "\n",
    "$$\n",
    "y = \\tilde{\\mathbf X} \\cdot \\tilde{\\mathbf A} + \\xi\n",
    "$$\n",
    "\n",
    "と表すこともできる．\n",
    "\n",
    "ノイズはしばしば平均 $0$ の正規分布 $\\mathrm N(0, \\sigma^2)$ に従う確率変数とみなして用いる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単回帰のとき（復習）\n",
    "\n",
    "回帰変数1つの単回帰では，データ空間 $(x, y)$ の2次元平面に回帰直線を引き，\n",
    "データから直線までの高さの二乗和が最小になるようにパラメータを選んだ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真のパラメータ\n",
    "A0 = 1.2\n",
    "A1 = 2.6\n",
    "\n",
    "# dataset\n",
    "X = np.arange(0, 3, 0.3)\n",
    "Y = A0 + A1*X + np.random.randn(X.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回帰直線のプロット\n",
    "def plot_XY_and_regressionline(a0=0.0, a1=1.0):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel(\"X\", size=20)\n",
    "    ax.set_ylabel(\"Y\", size=20)\n",
    "    ax.set_xticks\n",
    "    ax.set_ylim(-1, 9)\n",
    "    ax.scatter(X, Y)\n",
    "    ax.plot([np.min(X), np.max(X)], [a0+a1*np.min(X), a0+a1*np.max(X)], linewidth=3, color='tab:red')\n",
    "    ax.set_title('MSE = %f'%(np.sum((Y-a0-a1*X)**2)/Y.size), size=20)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    for x, y in zip(X, Y):\n",
    "        \n",
    "        ypred = a0 + a1*x\n",
    "        sq = patches.Rectangle(xy=(x, ypred), width=np.abs(y-ypred), height=np.abs(y-ypred), ec='k', fc='lime', alpha=0.8)\n",
    "        ax.add_patch(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_XY_and_regressionline, a0=(-5.0, 5.0, 0.1), a1=(-5.0, 10.0, 0.1))\n",
    "\n",
    "# 青点がデータ点\n",
    "# 赤が回帰直線\n",
    "# 緑の長方形が二乗誤差：長方形の総面積が誤差二乗和\n",
    "# MSE = Mean Squared Error = 平均二乗誤差\n",
    "\n",
    "# 必ずしも真のパラメータのときに平均二乗誤差最小になるわけではないことに注意しよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重回帰では\n",
    "\n",
    "回帰変数2つの重回帰では，データ空間 $(x_1, x_2, y)$ の3次元空間に回帰平面を引き，データから平面までの高さの二乗和が最小になるように選ぶ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真のパラメータ\n",
    "B0 = 1.2\n",
    "B1 = 1.8\n",
    "B2 = 0.2\n",
    "\n",
    "# dataset\n",
    "N = 1000\n",
    "X1 = np.random.rand(N) * 3\n",
    "X2 = np.random.rand(N) * 3\n",
    "Y = B0 + B1*X1 + B2*X2 + np.random.randn(X1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回帰平面のプロット\n",
    "def plot_X1X2Y_and_regressionplane(b0=0.0, b1=0.0, b2=0.0, azimuth=30, elevation=30):\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "    ax.view_init(azim=azimuth, elev=elevation)    \n",
    "    ax.set_xlabel(\"X1\")\n",
    "    ax.set_ylabel(\"X2\")\n",
    "    ax.set_zlabel(\"Y\")\n",
    "    \n",
    "    ax.set_zlim(-1, 9)\n",
    "    ax.scatter(X1, X2, Y, s=200)\n",
    "    \n",
    "    X2mesh = X1mesh = np.arange(0, 3, 0.3)\n",
    "    \n",
    "    X1mesh, X2mesh = np.meshgrid(X1mesh, X2mesh)\n",
    "    Ymesh = b0 + b1*X1mesh + b2*X2mesh\n",
    "    ax.plot_wireframe(X1mesh, X2mesh, Ymesh, linewidth=3, color='tab:red')\n",
    "    ax.set_title('MSE = %f'%(np.sum((Y-b0-b1*X1-b2*X2)**2)/Y.size), size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_X1X2Y_and_regressionplane, b0=(-5.0, 5.0, 0.1), b1=(-5.0, 10.0, 0.1), b2=(-5.0, 10.0, 0.1), azimuth=(0, 360, 1), elevation=(0, 90, 1))\n",
    "\n",
    "# 青点がデータ点\n",
    "# 赤が回帰平面\n",
    "# MSE = Mean Squared Error = 平均二乗誤差\n",
    "\n",
    "# 必ずしも真のパラメータのときに平均二乗誤差最小になるわけではないことに注意しよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習方法\n",
    "\n",
    "上では手動で回帰係数を動かして回帰モデルを作った．最適なパラメータを計算してみよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### モデル推定値\n",
    "\n",
    "$N$ 組みのデータセット \n",
    "\n",
    "$$\n",
    "\\left\\{ (x_1^{(i)}, x_2^{(i)}, \\ldots, x_M^{(i)}, y^{(i)}) \\right\\}_{i=1}^N \\quad \\left( = \\left\\{ (\\mathbf x^{(i)}, y^{(i)}) \\right\\}_{i=1}^N \\right)\n",
    "$$\n",
    "\n",
    "があるとしよう．パラメータ $\\mathbf A$ で作った重回帰モデルの推定値は\n",
    "\n",
    "$$\n",
    "a_0 + \\sum_{j=1}^M a_j x_j^{(i)} + \\xi^{(i)}\n",
    "$$\n",
    "\n",
    "である．ここで $\\xi^{(i)}$ は $i$ 番目のデータに対するランダムさ $\\xi$ の実現値である．\n",
    "\n",
    "$\\xi$ は平均 $0$ だったので，推定値の期待値は\n",
    "\n",
    "$$\n",
    "a_0 + \\sum_{j=1}^M a_j x_j^{(i)}\n",
    "$$\n",
    "\n",
    "である．\n",
    "\n",
    "---\n",
    "### 回帰残差\n",
    "\n",
    "したがって，パラメータ $\\mathbf A$ で作った重回帰モデルによって得られる $i$ 番目のデータに対する推定値の誤差（回帰残差）の期待値は\n",
    "\n",
    "$$\n",
    "e^{(i)} := y^{(i)} - a_0 - \\sum_{j=1}^M a_j x_j^{(i)}\n",
    "$$\n",
    "\n",
    "である．\n",
    "\n",
    "---\n",
    "### 平均二乗誤差\n",
    "\n",
    "誤差平方（二乗誤差）は\n",
    "\n",
    "$$\n",
    "(e^{(i)})^2 = \\left( y^{(i)} - a_0 - \\sum_{j=1}^M a_j x_j^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "なので，平均二乗誤差は，$N$ 個のデータセットに対する誤差平方の平均値であるから，\n",
    "\n",
    "$$\n",
    "E(a_0, a_1, \\ldots, a_N) := \\frac1N \\sum_{i=1}^N (e^{(i)})^2 = \\frac1N \\sum_{i=1}^N \\left( y^{(i)} - a_0 - \\sum_{j=1}^M a_j x_j^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "となる．\n",
    "\n",
    "平均二乗誤差は，モデルのパラメータ $\\tilde{\\mathbf A} = (a_0, a_1, \\ldots, a_M)$ が変わるごとに変化するので，$a_0, a_1, \\ldots, a_M$ の関数 $E(a_0, a_1, \\ldots, a_M)$ として置くことにする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 最小二乗法\n",
    "\n",
    "重回帰モデルのパラメータを決定するために，平均二乗誤差を最小にするパラメータを求めることにする．これを最小二乗法と呼ぶ．（平均二乗誤差最小以外のパラメータフィッティングの方法もある．）\n",
    "\n",
    "つまり，平均二乗誤差の関数 $E(a_0, a_1, \\ldots, a_M)$ が最小の値となるパラメータ $a_0, a_1, \\ldots, a_M$ を求める．計算方法は単純に多変数関数の極値問題を解けば良い．\n",
    "\n",
    "極値の候補を求めるには，$a_0, a_1, \\ldots, a_M$ それぞれの変数に関する偏導関数を計算して，全ての偏導関数の値が $0$ になる点（勾配が零ベクトルとなる点）を求めれば良い．\n",
    "\n",
    "---\n",
    "### 回帰残差の勾配\n",
    "\n",
    "先に回帰残差について偏導関数を計算しておこう．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\frac{\\partial e^{(i)}}{\\partial a_0} = \\frac{\\partial}{\\partial a_0} \\left( y^{(i)} - a_0 - \\sum_{j=1}^M a_j x_j^{(i)} \\right) = -1,\n",
    "\\\\\n",
    "(k=1, 2, \\ldots, M) \\quad & \\frac{\\partial e^{(i)}}{\\partial a_k} = \\frac{\\partial}{\\partial a_k} \\left( y^{(i)} - a_0 - \\sum_{j=1}^M a_j x_j^{(i)} \\right) = -\\sum_{j=1}^M \\frac{\\partial}{\\partial a_k} a_j x_j^{(i)} = -\\sum_{j=1}^M \\delta_{jk} x_j^{(i)}\n",
    "= -x_k^{(i)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "### 平均二乗誤差の勾配\n",
    "\n",
    "これにより平均二乗誤差の偏導関数は\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\frac{\\partial E}{\\partial a_0} = \\frac{\\partial}{\\partial a_0} \\frac1N \\sum_{i=1}^N (e^{(i)})^2\n",
    "= \\frac2N \\sum_{i=1}^N e^{(i)} \\frac{\\partial e^{(i)}}{\\partial a_0} = -\\frac2N \\sum_{i=1}^N e^{(i)} \\\\\n",
    "& \\phantom{\\frac{\\partial E}{\\partial a_0}} = -\\frac2N \\sum_{i=1}^N \\left( y^{(i)} - a_0 - \\sum_{j=1}^M a_j x_j^{(i)} \\right) = -2 \\left\\{ \\frac1N \\sum_{i=1}^N y^{(i)} - a_0 - \\sum_{j=1}^M a_j \\left( \\frac1N \\sum_{i=1}^N x_j^{(i)} \\right) \\right\\} \\\\\n",
    "& \\phantom{\\frac{\\partial E}{\\partial a_0}} := -2 \\left( \\langle y \\rangle - a_0 - \\sum_{j=1}^M a_j \\langle x_j \\rangle \\right),\n",
    "\\\\\n",
    "(k=1, 2, \\ldots, M) \\quad & \\frac{\\partial E}{\\partial a_k} = \\frac{\\partial}{\\partial a_k} \\frac1N \\sum_{i=1}^N (e^{(i)})^2\n",
    "= \\frac2N \\sum_{i=1}^N e^{(i)} \\frac{\\partial e^{(i)}}{\\partial a_k} = -\\frac2N \\sum_{i=1}^N e^{(i)} x_k^{(i)} = \\frac2N \\sum_{i=1}^N e^{(i)} \\frac{\\partial e^{(i)}}{\\partial a_k} = -\\frac2N \\sum_{i=1}^N e^{(i)} x_k^{(i)} \\\\\n",
    "& \\phantom{\\frac{\\partial E}{\\partial a_k}} = -\\frac2N \\sum_{i=1}^N \\left( y^{(i)} - a_0 - \\sum_{j=1}^M a_j x_j^{(i)} \\right) x_k^{(i)} = -2 \\left\\{ \\frac1N \\sum_{i=1}^N y^{(i)} x_k^{(i)} - a_0 \\frac1N \\sum_{i=1}^N x_k^{(i)} - \\sum_{j=1}^M a_j \\left( \\frac1N \\sum_{i=1}^N x_j^{(i)} x_k^{(i)} \\right) \\right\\} \\\\\n",
    "& \\phantom{\\frac{\\partial E}{\\partial a_0}} := -2 \\left( \\langle y x_k \\rangle - a_0 \\langle x_k \\rangle - \\sum_{j=1}^M a_j \\langle x_k x_j \\rangle \\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "となる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 平均二乗誤差の極値\n",
    "\n",
    "$\\frac{\\partial E}{\\partial a_k}=0$ が全ての $k=0, 1, 2, \\ldots, M$ で成り立つとすると\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\n",
    "\\begin{cases}\n",
    "\\displaystyle \\frac{\\partial E}{\\partial a_0} = 0 \\\\[5pt]\n",
    "\\displaystyle \\frac{\\partial E}{\\partial a_k} = 0 & (k = 1, 2, \\ldots, M)\n",
    "\\end{cases}\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "\\begin{cases}\n",
    "\\displaystyle \\langle y \\rangle - a_0 - \\sum_{j=1}^M a_j \\langle x_j \\rangle = 0 \\\\\n",
    "\\displaystyle \\langle y x_k \\rangle - a_0 \\langle x_k \\rangle - \\sum_{j=1}^M a_j \\langle x_k x_j \\rangle = 0  & (k = 1, 2, \\ldots, M)\n",
    "\\end{cases}\n",
    "\\\\\n",
    "&\n",
    "\\Longleftrightarrow \\quad\n",
    "\\begin{cases}\n",
    "\\displaystyle a_0 + \\sum_{j=1}^M a_j \\langle x_j \\rangle = \\langle y \\rangle \\\\\n",
    "\\displaystyle a_0 \\langle x_k \\rangle + \\sum_{j=1}^M a_j \\langle x_k x_j \\rangle = \\langle y x_k \\rangle  & (k = 1, 2, \\ldots, M)\n",
    "\\end{cases}\n",
    "\\\\\n",
    "&\n",
    "\\Longleftrightarrow \\quad\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "1 & \\langle x_1 \\rangle & \\langle x_2 \\rangle & \\ldots & \\langle x_M \\rangle \\\\\n",
    "\\langle x_1 \\rangle & \\langle (x_1)^2 \\rangle & \\langle x_1 x_2 \\rangle & \\ldots & \\langle x_1 x_M \\rangle \\\\\n",
    "\\langle x_2 \\rangle & \\langle x_1 x_2 \\rangle & \\langle (x_2)^2 \\rangle & \\ldots & \\langle x_2 x_M \\rangle \\\\\n",
    "\\vdots & \\vdots & & \\ddots & \\vdots \\\\\n",
    "\\langle x_M \\rangle & \\langle x_M x_2 \\rangle & \\langle x_M x_2 \\rangle & \\ldots & \\langle (x_M)^2 \\rangle\n",
    "\\end{pmatrix}\n",
    "}_{=: \\mathbb M \\\\ \\scriptsize \\mbox{given by data}}\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "a_0 \\\\ a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_M\n",
    "\\end{pmatrix}\n",
    "}_{= \\tilde{\\mathbf A}^{\\mathrm T}\\\\ \\scriptsize \\mbox{parameters} \\\\ \\scriptsize \\mbox{to be fitted}}\n",
    "=\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\langle y \\rangle \\\\ \\langle y x_1 \\rangle \\\\ \\langle y x_2 \\rangle \\\\ \\vdots \\\\ \\langle y x_M \\rangle\n",
    "\\end{pmatrix}\n",
    "}_{=: \\mathbf B \\\\ \\scriptsize\\mbox{given by data}}\n",
    "\\\\\n",
    "&\n",
    "\\Longleftrightarrow \\quad\n",
    "\\mathbb M \\tilde{\\mathbf A}^{\\mathrm T} = \\mathbf B\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "という連立一次方程式の解となることがわかる．\n",
    "\n",
    "ただし，\n",
    "\n",
    "- $\\displaystyle \\langle y \\rangle = \\frac1N \\sum_{i=1}^M y^{(i)}$ は目的変数 $y$ の標本平均\n",
    "- $\\displaystyle \\langle x_k \\rangle = \\frac1N \\sum_{i=1}^M x_k^{(i)}$ $(k=1, 2, \\ldots, M)$ は各回帰変数 $x_k$ の標本平均\n",
    "- $\\displaystyle \\langle x_k x_j \\rangle = \\frac1N \\sum_{i=1}^M x_j^{(i)} x_k^{(i)}$ $(j, k=1, 2, \\ldots, M)$ は回帰変数 $x_k$ と $x_j$ の積の標本平均\n",
    "- $\\displaystyle \\langle y x_k \\rangle = \\frac1N \\sum_{i=1}^M y x_k^{(i)}$ $(j, k=1, 2, \\ldots, M)$ は目的変数 $y$ と回帰変数 $x_k$ の積の標本平均\n",
    "\n",
    "この連立一次方程式の係数行列 $\\mathbb M$ は非負定値対称行列（固有値は全て非負）である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### グラム行列\n",
    "\n",
    "$N \\times (M+1)$ 行列を以下のように\n",
    "\n",
    "$$\n",
    "\\mathbb X = \n",
    "\\begin{pmatrix}\n",
    "\\tilde{\\mathbf X}^{(1)} \\\\\n",
    "\\tilde{\\mathbf X}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "\\tilde{\\mathbf X}^{(N)}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} & \\ldots & x_M^{(1)} \\\\\n",
    "1 & x_1^{(2)} & x_2^{(2)} & \\ldots & x_M^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "1 & x_1^{(N)} & x_2^{(N)} & \\ldots & x_M^{(N)} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "置くと，上の連立一次方程式の係数行列は\n",
    "\n",
    "$$\n",
    "\\mathbb M = \\frac1N \\mathbb X^{\\mathrm T} \\mathbb X\n",
    "$$\n",
    "\n",
    "と表すことができる．ただし ${}^{\\mathrm T}$ は転置を表す．$\\mathbb X^{\\mathrm T} \\mathbb X$ の形の行列を $\\mathbb X$ の__グラム行列__という．\n",
    "\n",
    "$\\mathbb M$ は，$\\mathbb X$ のグラム行列ではなく，正確には $\\frac1{\\sqrt N}\\mathbb X$ のグラム行列であるが，以下では $\\mathbb M$ もグラム行列とよぶことにする．\n",
    "\n",
    "$N\\times(M+1)$-実行列 $\\mathbb X$ のグラム行列に対して一般に成り立つ重要な性質としては\n",
    "\n",
    "- $\\mathbb X^{\\mathrm T} \\mathbb X$ は非負定値対称行列\n",
    "- $\\mathbb X^{\\mathrm T} \\mathbb X$ の階数は $\\mathbb X$ の階数に等しい．つまり $\\mathrm{rank}(\\mathbb X^{\\mathrm T} \\mathbb X) = \\mathrm{rank}(\\mathbb X)$\n",
    "- $\\mathbb X^{\\mathrm T} \\mathbb X$ が正定値となるための必要十分条件は $\\mathbb X$ の階数 (rank) が $M+1$ となること（つまり $\\mathbb X$ は full rank）\n",
    "\n",
    "グラム行列が正定値になるには，データ数が回帰変数の数 $M$ を上回る必要がある．（もちろん上回るだけでは必ずしも正定値になるとは限らない．線型独立なインスタンスが $M+1$ 個必要ということ．）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### グラム行列の非負定値性（参考）\n",
    "\n",
    "$\\mathbb X$ を\n",
    "\n",
    "$$\n",
    "\\mathbb X = (\\mathbf x_0, \\mathbf x_1, \\mathbf x_2, \\ldots, \\mathbf x_M), \\quad\n",
    "\\mathbf x_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}, \\ \n",
    "\\mathbf x_j = \\begin{pmatrix} x_j^{(1)} \\\\ x_j^{(2)} \\\\ \\vdots \\\\ x_j^{(N)} \\end{pmatrix} \\ (j=1, 2, \\ldots, M)\n",
    "$$\n",
    "\n",
    "と $N$ 次の列ベクトル $M+1$ 個に分割しておくと，グラム行列 $\\mathbb M$ の $(i, j)$ 成分 $M_{ij}$ $(i, j = 1, 2, \\ldots, M+1)$ は\n",
    "\n",
    "$$\n",
    "M_{ij} = \\frac1N \\mathbf x_{i+1}^{\\mathrm T} \\mathbf x_{j+1} \\quad \\left( = \\langle x_{i+1} x_{j+1} \\rangle \\right)\n",
    "$$\n",
    "\n",
    "というように，$\\mathbb X$ の列ベクトルの内積で表される．\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "データを実数値に限っておくと，$\\mathbb X^{\\mathrm T} \\mathbb X$ は実対称行列なので適当な直交行列 $\\mathbb U$ を用いて\n",
    "\n",
    "$$\n",
    "\\mathbb U^{\\mathrm T} \\mathbb M \\mathbb U = \\mathbb U^{\\mathrm T} (\\mathbb X^{\\mathrm T} \\mathbb X) \\mathbb U \n",
    "= \\begin{pmatrix}\n",
    "\\lambda_0 &  & & O \\\\ & \\lambda_1 \\\\ & & \\ddots \\\\ O & & & \\lambda_M\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "と対角行列に変形できる．\n",
    "\n",
    "$\\mathbb X \\mathbb U =: \\mathbb V$ と置き，$\\mathbb V$ を\n",
    "\n",
    "$$\n",
    "\\mathbb V = (\\mathbf v_0, \\mathbf v_1, \\ldots, \\mathbf v_M)\n",
    "$$\n",
    "\n",
    "のように列ベクトル分割をしておくと，上の左辺は\n",
    "\n",
    "$$\n",
    "\\mathbb U^{\\mathrm T} (\\mathbb X^{\\mathrm T} \\mathbb X) \\mathbb U \n",
    "= (\\mathbb X \\mathbb U)^{\\mathrm T} (\\mathbb X \\mathbb U) = \\mathbb V^{\\mathrm T} \\mathbb V\n",
    "= \\begin{pmatrix} \n",
    "\\mathbf v_0^{\\mathrm T} \\mathbf v_0 & \\mathbf v_0^{\\mathrm T} \\mathbf v_1 & \\ldots & \\mathbf v_0^{\\mathrm T} \\mathbf v_M \\\\\n",
    "\\mathbf v_1^{\\mathrm T} \\mathbf v_0 & \\mathbf v_1^{\\mathrm T} \\mathbf v_1 & \\ldots & \\mathbf v_1^{\\mathrm T} \\mathbf v_M \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\\n",
    "\\mathbf v_M^{\\mathrm T} \\mathbf v_0 & \\mathbf v_M^{\\mathrm T} \\mathbf v_1 & \\ldots & \\mathbf v_M^{\\mathrm T} \\mathbf v_M \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "となる．両辺比較すると\n",
    "\n",
    "$$\n",
    "\\mathbf v_i^{\\mathrm T} \\mathbf v_j = \\delta_{ij} \\lambda_i,\n",
    "$$\n",
    "\n",
    "特に\n",
    "\n",
    "$$\n",
    "\\lambda_i = \\| \\mathbf v_i \\|^2 \\ge 0\n",
    "$$\n",
    "\n",
    "とわかる．ただし $\\| \\cdot \\|$ はユークリッドノルム（2-ノルム）である．\n",
    "\n",
    "したがって係数行列 $\\mathbb X^{\\mathrm T} \\mathbb X$ の固有値は全て非負とわかった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### グラム行列が正定値になるとき\n",
    "\n",
    "$\\mathbb X^{\\mathrm T}\\mathbb X$ は $(M+1)$ 次正方行列であり，\n",
    "\n",
    "$$\n",
    "\\mathrm{rank}{\\mathbb X^{\\mathrm T}\\mathbb X} = \\mathrm{rank}(\\mathbb X)\n",
    "$$\n",
    "\n",
    "であることから，$\\mathrm{rank}(\\mathbb X) = M+1$ であることが，$\\mathbb X^{\\mathrm T}\\mathbb X$ の正定値性の必要十分条件であることがわかる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\mathbb X$ が full rank のときの最適パラメータ\n",
    "\n",
    "$\\mathbb X$ が full rank であるとしよう．このとき，連立一次方程式の係数行列 $\\mathbb M$ は正則なので，連立一次方程式 $\\mathbb M \\tilde{\\mathbf A}^{\\mathrm T} = \\mathbf B$ の解はただ一つである．つまり，平均二乗誤差を与える関数 $E(a_0, a_1, \\ldots, a_N)$ の極値の候補は一つだけということがわかる．実際にこの点で極小点を与え，さらに最小であることもわかる．そのパラメータ点は\n",
    "\n",
    "$$\n",
    "\\mathbb M \\tilde{\\mathbf A}^{\\mathrm T} = \\mathbf B \\ \\Longleftrightarrow \\ \n",
    "\\tilde{\\mathbf A}^{\\mathrm T} = \\mathbb M^{-1} \\mathbf B\n",
    "$$\n",
    "\n",
    "と表される．この $\\tilde{\\mathbf A}^{\\mathrm T} = \\mathbb M^{-1} \\mathbf B$ で定められるパラメータ $\\tilde{\\mathbf A} = (a_0, a_1, a_2, \\ldots, a_M)$ が，重回帰モデルにおいて平均二乗誤差最小を与えるパラメータである．\n",
    "\n",
    "ここで\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf A}^{\\mathrm T}_* = \\mathbb M^{-1} \\mathbf B\n",
    "$$\n",
    "\n",
    "と区別しておこう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\tilde{\\mathbf A}^{\\mathrm T}_* = \\mathbb M^{-1} \\mathbf B$ において極小を取ること（参考）\n",
    "\n",
    "$\\tilde{\\mathbf A}^{\\mathrm T}_* = \\mathbb M^{-1} \\mathbf B$ が極小値となることを示すには，$E(\\tilde{\\mathbf A})$ のヘッセ行列が $\\tilde{\\mathbf A}^{\\mathrm T}_* = \\mathbb M^{-1} \\mathbf B$ において正定値であることを示せばよい．\n",
    "\n",
    "ヘッセ行列 $\\mathbb H = (H_{kl})$ の $(k,l)$ 成分は $\\frac{\\partial^2 E}{\\partial a_k \\partial a_l}$ で与えられる．すると\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& H_{11} = \\frac{\\partial^2 E}{\\partial (a_0)^2} = -2 \\frac{\\partial}{\\partial a_0} \\left( \\langle y \\rangle - a_0 - \\sum_{j=1}^M a_j \\langle x_j \\rangle \\right) = 2 = 2 M_{11},\n",
    "\\\\\n",
    "(k = 1, 2, \\ldots, M) \\quad & H_{1,k+1} = \\frac{\\partial^2 E}{\\partial a_0 \\partial a_k} = -2 \\frac{\\partial}{\\partial a_k} \\left( \\langle y \\rangle - a_0 - \\sum_{j=1}^M a_j \\langle x_j \\rangle \\right) = 2 \\langle x_k \\rangle = 2M_{1, k+1},\n",
    "\\\\\n",
    "(k = 1, 2, \\ldots, M) \\quad & H_{k+1, 1} = H_{1, k+1} = 2 \\langle x_k \\rangle = 2M_{k+1, 1},\n",
    "\\\\\n",
    "(k, l = 1, 2, \\ldots, M) \\quad & \\frac{\\partial^2 E}{\\partial a_k \\partial a_l} = -2 \\frac{\\partial}{\\partial a_l} \\left( \\langle y x_k \\rangle - a_0 \\langle x_k \\rangle - \\sum_{j=1}^M a_j \\langle x_k x_j \\rangle \\right) = 2 \\langle x_k x_l \\rangle = 2 M_{kl}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "となることから，\n",
    "\n",
    "$$\n",
    "\\mathbb H = 2 \\mathbb M\n",
    "$$\n",
    "\n",
    "とわかる．$\\mathbb H$ は $a_0, a_1, \\ldots, a_M$ には依存しない定行列である．\n",
    "\n",
    "$\\mathbb M$ が正定値であれば $\\mathbb H$ も正定値となるので，$\\tilde{\\mathbf A}^{\\mathrm T}_* = \\mathbb M^{-1} \\mathbf B$ は $E(a_0, a_1, \\ldots, a_M)$ の極小を与える．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 最小値となること\n",
    "\n",
    "$\\mathbb X$ が full rank のとき，$E(a_0, a_1, \\ldots, a_M)$ のヘッセ行列 $\\mathbb H$ は全ての点で正定値なので，$E(a_0, a_1, \\ldots, a_M)$ は狭義凸関数である．\n",
    "\n",
    "狭義凸関数の極小点は最小点となるため，上で得られた $\\tilde{\\mathbf A}_*$ は $E(a_0, a_1, \\ldots, a_M)$ の最小を与えるパラメータであることがわかる．\n",
    "\n",
    "---\n",
    "### 凸関数の補足（参考）\n",
    "\n",
    "ここで $n$ 変数関数 $f$ が（広義）凸関数とは，\n",
    "\n",
    "$$\n",
    "f((1-c)\\mathbf x+c\\mathbf y) \\le (1-c)f(\\mathbf x) + c f(\\mathbf y)\n",
    "$$\n",
    "\n",
    "が全ての $\\mathbf x, \\mathbf y \\in \\mathbb R^n$ と $0<c<1$ なる全ての $c$ に対して成り立つときをいう．\n",
    "\n",
    "この不等号において等号が成り立つのが $\\mathbf x = \\mathbf y$ のときだけであるとき，特に狭義凸関数とよぶ．\n",
    "\n",
    "$f$ が狭義凸となる十分条件として，$f$ のヘッセ行列が全ての点で正定値となることが知られている．したがって，$E(a_0, a_1, \\ldots, a_M)$ は狭義凸関数である．\n",
    "\n",
    "凸関数のもう一つの重要な性質は\n",
    "\n",
    "$$\n",
    "f(\\mathbf a) + \\nabla f(\\mathbf a) (\\mathbf x - \\mathbf a) \\le f(\\mathbf x)\n",
    "$$\n",
    "\n",
    "が全ての $\\mathbf a, \\mathbf x \\in \\mathbb R^N$ において成り立つということ．\n",
    "\n",
    "これを $E(a_0, a_1, \\ldots, a_M)$ に適用すると，\n",
    "\n",
    "$$\n",
    "E(\\tilde{\\mathbf A}_*) + \\nabla E(\\tilde{\\mathbf A}_*) (\\tilde{\\mathbf A} - \\tilde{\\mathbf A}_*) \\le E(\\tilde{\\mathbf A})\n",
    "$$\n",
    "\n",
    "が全ての $\\tilde{\\mathbf A} \\in \\mathbb R^{M+1}$ に対して成り立つ．\n",
    "$\\tilde{\\mathbf A}_*$ は $E$ の極値なので $\\nabla E(\\tilde{\\mathbf A}_*)= \\mathbf 0$ が成り立つ．したがって\n",
    "\n",
    "$$\n",
    "E(\\tilde{\\mathbf A}_*) \\le E(\\tilde{\\mathbf A})\n",
    "$$\n",
    "\n",
    "となるため，$E(\\tilde{\\mathbf A}_*)$ が最小値であることがわかる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## では計算してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真のパラメータ\n",
    "B0 = 1.2\n",
    "B1 = 1.8\n",
    "B2 = 0.2\n",
    "\n",
    "# dataset\n",
    "N = 10\n",
    "X1 = np.random.rand(N) * 3\n",
    "X2 = np.random.rand(N) * 3\n",
    "Y = B0 + B1*X1 + B2*X2 + np.random.randn(X1.size)\n",
    "\n",
    "# この場合では N = 10, M = 2 である\n",
    "M = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb X = \n",
    "\\begin{pmatrix}\n",
    "\\tilde{\\mathbf X}^{(1)} \\\\\n",
    "\\tilde{\\mathbf X}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "\\tilde{\\mathbf X}^{(N)}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} \\\\\n",
    "1 & x_1^{(2)} & x_2^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "1 & x_1^{(N)} & x_2^{(N)} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ行列\n",
    "XX = np.ones((N, M+1))\n",
    "\n",
    "# XX の第0列は全て1，第1列は X1, 第2列は X2\n",
    "XX[:, 1] = X1\n",
    "XX[:, 2] = X2\n",
    "\n",
    "# M = (X^T)X / N\n",
    "MM = np.dot(XX.T, XX) / N # 行列の積には np.dot を用いる．XX.T で XX の転置が取れる\n",
    "\n",
    "BB = np.zeros(M+1)\n",
    "BB[0] = np.mean(Y)\n",
    "BB[1] = np.mean(Y*X1)\n",
    "BB[2] = np.mean(Y*X2)\n",
    "\n",
    "# 連立一次方程式 MA = B を解く\n",
    "AA = np.linalg.solve(MM, BB)\n",
    "\n",
    "print('最小二乗法で得られた重回帰モデル： %9.6f + %9.6f * x1 + %9.6f * x2'%(AA[0], AA[1], AA[2]))\n",
    "print('真の重回帰モデル：                         %9.6f + %9.6f * x1 + %9.6f * x2'%(B0, B1, B2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習で得られたパラメータは必ずしも真のパラメータに近いとは言えない．（ただし，与えられたデータセットに対して平均二乗誤差が最小であることは間違いない．）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "scikit-learn の結果と一致することを確かめてみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "X_train = XX[:, 1:3]\n",
    "Y_train = Y\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "print('scikit-learn の LinearRegression で得られた重回帰モデル： %9.6f + %9.6f * x1 + %9.6f * x2'\n",
    "      %(reg.intercept_, reg.coef_[0], reg.coef_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数 $N$ を変えてフィッテイングの精度を調べてみよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真のパラメータ\n",
    "B0 = 1.2\n",
    "B1 = 1.8\n",
    "B2 = 0.2\n",
    "\n",
    "# 回帰変数の数\n",
    "M = 2\n",
    "\n",
    "# データ数の常用対数の最大\n",
    "Lmax = 7\n",
    "B0_pred = np.zeros(Lmax)\n",
    "B1_pred = np.zeros(Lmax)\n",
    "B2_pred = np.zeros(Lmax)\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "for L in range(1, Lmax):\n",
    "    # データ数を変える\n",
    "    N = 10**L\n",
    "    \n",
    "    # dataset\n",
    "    X1 = np.random.rand(N) * 3\n",
    "    X2 = np.random.rand(N) * 3\n",
    "    Y = B0 + B1*X1 + B2*X2 + np.random.randn(X1.size)\n",
    "    \n",
    "    X_train = np.zeros((N, M))\n",
    "    X_train[:, 0] = X1\n",
    "    X_train[:, 1] = X2\n",
    "    \n",
    "    # sklearn の LinearRegression で fitting\n",
    "    reg.fit(X_train, Y)\n",
    "    B0_pred[L] = reg.intercept_\n",
    "    B1_pred[L] = reg.coef_[0]\n",
    "    B2_pred[L] = reg.coef_[1]\n",
    "    print('scikit-learn の LinearRegression で得られた重回帰モデル： %9.6f + %9.6f * x1 + %9.6f * x2'\n",
    "          %(reg.intercept_, reg.coef_[0], reg.coef_[1]))    \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel(\"L\", size=20)\n",
    "ax.plot( np.arange(1, Lmax+1, 1), np.log10(np.abs(B0-B0_pred)), 'o-', label='$\\log|B0 - B0_{pred}|$')\n",
    "ax.plot( np.arange(1, Lmax+1, 1), np.log10(np.abs(B1-B1_pred)), 'o-', label='$\\log|B1 - B1_{pred}|$')\n",
    "ax.plot( np.arange(1, Lmax+1, 1), np.log10(np.abs(B2-B2_pred)), 'o-', label='$\\log|B2 - B2_{pred}|$')\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習問題 3-1\n",
    "\n",
    "第2回その3で Iris データセットを用いて線形回帰モデルを適用した．そこでは目的変数として `Petal Length` を取り，単回帰では説明変数に `Sepal Length` を，重回帰では説明変数に `Sepal Length` と `Petal Width` を用いてモデルを立てた．さらには `Species` でデータを分けて種別に回帰することでよりフィッティングが合うようにした．\n",
    "\n",
    "では，目的変数と説明変数を全ての組み合わせを取り替えて線形回帰モデルを試し，回帰変数の数とフィッティング精度に傾向があるか確かめよ．（植物学的に意味があるかはさておき）一番フィッティングの合う線形回帰モデルは何か．その理由も考えよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div style=\"text-align: right;\">その2につづく</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
