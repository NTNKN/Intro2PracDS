{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実践データ科学入門 2020年度木曜4限\n",
    "\n",
    "# 第3回 その2 多項式回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook # if necessary to rotate figures in 3D plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import art3d\n",
    "from ipywidgets import interact\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多項式回帰とは\n",
    "\n",
    "多項式回帰は説明変数 $x$ 1つの場合に，$x$ による単回帰ではなく，$x$, $x^2$, $x^3$ など，$x$ のべき乘の功を回帰変数に用いる手法で，\n",
    "\n",
    "$$\n",
    "y = a_0 + a_1 x + a_2 x^2 + \\ldots + a_m x^m = \\sum_{j=0}^M a_j x^j + \\xi\n",
    "$$\n",
    "\n",
    "というモデルを当てはめる方法である．多項式回帰モデルは，回帰変数を $x_j = x^j$ とした重回帰モデルであると言える．\n",
    "\n",
    "データに対して単回帰を用いて直線を当てはめるのが不適切とわかる場合に用いられる．\n",
    "\n",
    "ここで\n",
    "\n",
    "- $y$ は目的変数（データとして与えられるもの）\n",
    "- $x^j \\ (j=0, 1, 2, \\ldots, M)$ を説明変数と取る（データとして与えられるもの）\n",
    "- $a_j \\ (j=0, 1, 2, \\ldots, M)$ は回帰係数（データから求めるもの）\n",
    "- $\\xi$ はノイズ（モデルでは当てはめられないランダムな要因）\n",
    "\n",
    "以下，特に注意しない限り__データは実数値__とする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真のパラメータ\n",
    "A0 = 1.2\n",
    "A1 = -5.6\n",
    "A2 = 2.2\n",
    "\n",
    "# dataset\n",
    "X = np.arange(0, 3, 0.3)\n",
    "N = X.size\n",
    "Y = A0 + A1*X + A2*X**2 +0.5*np.random.randn(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回帰直線のプロット\n",
    "def plot_XY_and_regressionline(a0=0.0, a1=1.0):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel(\"X\", size=20)\n",
    "    ax.set_ylabel(\"Y\", size=20)\n",
    "    ax.set_xticks\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.scatter(X, Y)\n",
    "    ax.plot([np.min(X), np.max(X)], [a0+a1*np.min(X), a0+a1*np.max(X)], linewidth=3, color='tab:red')\n",
    "    ax.set_title('MSE = %f'%(np.sum((Y-a0-a1*X)**2)/Y.size), size=20)\n",
    "    ax.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_XY_and_regressionline, a0=(-5.0, 5.0, 0.1), a1=(-5.0, 10.0, 0.1))\n",
    "\n",
    "# 青点がデータ点\n",
    "# 赤が回帰直線\n",
    "# MSE = Mean Squared Error = 平均二乗誤差\n",
    "\n",
    "# 必ずしも真のパラメータのときに平均二乗誤差最小になるわけではないことに注意しよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直線でのフィッティングでは適切なモデルとは言えない．多項式フィッティングを考えよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多項式フィッティング\n",
    "\n",
    "回帰変数2つの重回帰では，データ空間 $(x_1, x_2, y)$ の3次元空間に回帰平面を引き，データから平面までの高さの二乗和が最小になるように選ぶ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真のパラメータ\n",
    "A0 = 1.2\n",
    "A1 = -5.6\n",
    "A2 = 2.2\n",
    "\n",
    "# dataset\n",
    "X = np.arange(0, 3, 0.3)\n",
    "Y = A0 + A1*X + A2*X**2 +0.5*np.random.randn(X.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フィッティングに用いる多項式の最大次数\n",
    "M = 2\n",
    "\n",
    "# dataset\n",
    "X_train = np.zeros((X.size, M))\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "for j in range(M):\n",
    "    X_train[:, j] = X**(j+1)\n",
    "\n",
    "reg.fit(X_train, Y)\n",
    "\n",
    "A_pred = np.zeros(M+1)\n",
    "A_pred[0] = reg.intercept_\n",
    "A_pred[1:M+1] = reg.coef_\n",
    "\n",
    "XX = np.arange(-0.2, 3, 0.1)\n",
    "YY = np.ones(XX.size)*A_pred[0]\n",
    "Y_pred = np.ones(X.size)*A_pred[0]\n",
    "for j in range(1, M+1):\n",
    "    YY += A_pred[j] * XX**j\n",
    "    Y_pred += A_pred[j] * X**j\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel(\"X\", size=20)\n",
    "ax.set_ylabel(\"Y\", size=20)\n",
    "ax.set_xticks\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.scatter(X, Y, s=80)\n",
    "ax.plot(XX, YY, linewidth=3, c='tab:orange')\n",
    "ax.set_title('MSE = %f'%(np.sum((Y-Y_pred)**2)/Y.size), size=20)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "print('A0 = %f'%(reg.intercept_))\n",
    "for j in range(1, M+1):\n",
    "    print('A%1d = %f'%(j, reg.coef_[j-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2次関数でうまくフィッティングができている．\n",
    "\n",
    "しかし，フィッティングに用いる多項式の次数を増やすと MSE をもっと下げることができる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多項式回帰のプロット\n",
    "def plot_polynomialregression(M=2):\n",
    "    # dataset\n",
    "    X_train = np.zeros((N, M))\n",
    "\n",
    "    reg = linear_model.LinearRegression()\n",
    "    for j in range(M):\n",
    "        X_train[:, j] = X**(j+1)\n",
    "\n",
    "    reg.fit(X_train, Y)\n",
    "\n",
    "    A_pred = np.zeros(M+1)\n",
    "    A_pred[0] = reg.intercept_\n",
    "    A_pred[1:M+1] = reg.coef_\n",
    "\n",
    "    XX = np.arange(-0.2, 3, 0.01)\n",
    "    YY = np.ones(XX.size)*A_pred[0]\n",
    "    Y_pred = np.ones(X.size)*A_pred[0]\n",
    "    for j in range(1, M+1):\n",
    "        YY += A_pred[j] * XX**j\n",
    "        Y_pred += A_pred[j] * X**j\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel(\"X\", size=20)\n",
    "    ax.set_ylabel(\"Y\", size=20)\n",
    "    ax.set_xticks\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.scatter(X, Y, s=80)\n",
    "    ax.plot(XX, YY, linewidth=3, c='tab:orange')\n",
    "    ax.set_title('MSE = %f'%(np.sum((Y-Y_pred)**2)/Y.size), size=20)\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    #print('A0 = %f'%(reg.intercept_))\n",
    "    #for j in range(1, M+1):\n",
    "    #    print('A%1d = %f'%(j, reg.coef_[j-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_polynomialregression, M=(1, 20, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多項式の次数をデータ数と比べて十分高く取ると，全ての点を通るような多項式曲線を描くことができる．この時はデータ点に対する回帰誤差は必ずゼロとなる．\n",
    "\n",
    "MSE だけを減らせば良いというわけではないことがわかる．\n",
    "\n",
    "このことを一般に過学習 (overfitting) という．回帰変数を増やせば，すなわちモデルを複雑にすれば MSE をいくらでも下げることができる一つの例である．\n",
    "\n",
    "過学習となっている場合は，学習データ近傍であっても回帰曲線は正しい推定値を与えない．推定誤差が低くなってしまう．このことを確かめるために同じ分布に従う別なデータセットを取って確かめてみよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用 dataset\n",
    "X2 = np.arange(0.1, 3, 0.3)\n",
    "Y2 = A0 + A1*X2 + A2*X2**2 +0.5*np.random.randn(X2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多項式回帰とテストデータの比較\n",
    "def plot_polynomialregression_fortestdata(M=2):\n",
    "    # dataset\n",
    "    X_train = np.zeros((N, M))\n",
    "\n",
    "    reg = linear_model.LinearRegression()\n",
    "    for j in range(M):\n",
    "        X_train[:, j] = X**(j+1)\n",
    "\n",
    "    reg.fit(X_train, Y)\n",
    "\n",
    "    A_pred = np.zeros(M+1)\n",
    "    A_pred[0] = reg.intercept_\n",
    "    A_pred[1:M+1] = reg.coef_\n",
    "\n",
    "    XX = np.arange(-0.2, 3, 0.01)\n",
    "    YY = np.ones(XX.size)*A_pred[0]\n",
    "    Y2_pred = np.ones(X2.size)*A_pred[0]\n",
    "    for j in range(1, M+1):\n",
    "        YY += A_pred[j] * XX**j\n",
    "        Y2_pred += A_pred[j] * X2**j\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel(\"X\", size=20)\n",
    "    ax.set_ylabel(\"Y\", size=20)\n",
    "    ax.set_xticks\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.scatter(X2, Y2, s=80)\n",
    "    ax.plot(XX, YY, linewidth=3, c='tab:orange')\n",
    "    ax.set_title('MSE = %f'%(np.sum((Y2-Y2_pred)**2)), size=20)\n",
    "    ax.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_polynomialregression_fortestdata, M=(1, 20, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに対する MSE は次数を上げるにつれて大きくなることがわかる．\n",
    "\n",
    "学習で得られたモデルをテストデータに適用して得られた回帰誤差を汎化誤差と呼ぶ．\n",
    "\n",
    "一方，学習時の回帰誤差を学習誤差と呼ぶ．\n",
    "\n",
    "学習誤差は多項式の次数を上げることでいくらでも下げられるが，汎化誤差はそうとは限らない．汎化誤差も小さいモデルが良いモデルであると言える．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mmax = 15\n",
    "MSE_train = np.zeros(Mmax+1)\n",
    "MSE_test = np.zeros(Mmax+1)\n",
    "for M in range(1, Mmax+1):\n",
    "    # dataset\n",
    "    X_train = np.zeros((X.size, M))\n",
    "\n",
    "    reg = linear_model.LinearRegression()\n",
    "    for j in range(M):\n",
    "        X_train[:, j] = X**(j+1)\n",
    "\n",
    "    reg.fit(X_train, Y)\n",
    "\n",
    "    A_pred = np.zeros(M+1)\n",
    "    A_pred[0] = reg.intercept_\n",
    "    A_pred[1:M+1] = reg.coef_\n",
    "\n",
    "    XX = np.arange(-0.2, 3, 0.01)\n",
    "    YY = np.ones(XX.size)*A_pred[0]\n",
    "    Y_pred = np.ones(X.size)*A_pred[0]\n",
    "    Y2_pred = np.ones(X2.size)*A_pred[0]\n",
    "    for j in range(1, M+1):\n",
    "        YY += A_pred[j] * XX**j\n",
    "        Y_pred += A_pred[j] * X**j\n",
    "        Y2_pred += A_pred[j] * X2**j\n",
    "\n",
    "    MSE_train[M] = np.sum((Y-Y_pred)**2)/Y.size\n",
    "    MSE_test[M] = np.sum((Y2-Y2_pred)**2)/Y.size\n",
    "\n",
    "# MSE のプロット\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel(\"M: degree of polynomial\", size=20)\n",
    "ax.set_ylabel(\"$\\log(MSE)$\", size=20)\n",
    "ax.set_ylim(-5, 6)\n",
    "ax.set_xticks(np.arange(1, Mmax+1, 1))\n",
    "ax.plot(np.arange(1, Mmax+1, 1), np.log10(MSE_train[1:]), 'o-', linewidth=3, c='tab:blue', label='training MSE')\n",
    "ax.plot(np.arange(1, Mmax+1, 1), np.log10(MSE_test[1:]), 'o-', linewidth=3, c='tab:orange', label='test MSE')\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "汎化誤差が最小になる次数を取ることで，過学習を防ぐことができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習問題 3-2\n",
    "\n",
    "第3回その1で Iris データセットに対して，あらゆる変数の組み合わせについて線形回帰モデルを試した．\n",
    "\n",
    "では，目的変数と説明変数を1つずつ取り，その組み合わせを取り替えて多項式回帰モデルを試し，多項式次数とフィッティング精度に傾向があるか確かめよ．上でやったように多項式次数を十分高く取ると過学習してしまうので，汎化誤差が小さくなるように多項式次数を調整し，学習誤差と汎化誤差の比較も行うこと．\n",
    "\n",
    "（植物学的に意味があるかはさておき）一番フィッティングの合う多項式回帰モデルは何か．その理由も考えよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div style=\"text-align: right;\">その3につづく</div></h3>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
