{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実践データ科学入門 2020年度木曜4限\n",
    "\n",
    "# 第4回 その1 Iris 線形回帰やりすぎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Irisデータセット読み込み\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# データの特徴量\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的変数 Y を PW  \n",
    "説明変数を SL, SW, PL として線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形回帰モデルの準備\n",
    "reg1 = linear_model.LinearRegression()\n",
    "reg2 = linear_model.LinearRegression()\n",
    "reg3 = linear_model.LinearRegression()\n",
    "\n",
    "# 説明変数と目的変数の設定\n",
    "Y = iris.data[:, 3]    # 目的変数はPW\n",
    "X1 = iris.data[:, 0:1] # case1: 説明変数はSLのみ\n",
    "X2 = iris.data[:, 0:2] # case2: 説明変数はSLとSW\n",
    "X3 = iris.data[:, 0:3] # case3: 説明変数はSL, SW, PL\n",
    "\n",
    "# 線形回帰\n",
    "reg1.fit(X1, Y)\n",
    "reg2.fit(X2, Y)\n",
    "reg3.fit(X3, Y)\n",
    "\n",
    "# 学習した回帰モデルで予測値を出力\n",
    "Y1p = reg1.predict(X1)\n",
    "Y2p = reg2.predict(X2)\n",
    "Y3p = reg3.predict(X3)\n",
    "\n",
    "# 学習誤差\n",
    "MSE1 = mean_squared_error(Y, Y1p)\n",
    "MSE2 = mean_squared_error(Y, Y2p)\n",
    "MSE3 = mean_squared_error(Y, Y3p)\n",
    "\n",
    "# MSEを表示\n",
    "print('case1: MSE = %f'%MSE1)\n",
    "print('case2: MSE = %f'%MSE2)\n",
    "print('case3: MSE = %f'%MSE3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仮に目的変数と説明変数の間に明確な因果関係がなく，偽相関しかなかったとしても  \n",
    "回帰変数を増やすと MSE は減る\n",
    "\n",
    "---\n",
    "\n",
    "実は，説明変数として乱数で生成したデータを回帰変数に増やしても MSE は減ってしまう  \n",
    "乱数で生成したデータなので，PW とは全く因果関係がないのに減ってしまう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 追加する回帰変数の数\n",
    "cols_max = 200\n",
    "\n",
    "# MSEを格納するための配列を準備\n",
    "MSEs = np.zeros(cols_max+3)\n",
    "\n",
    "# 最初の3つは上ですでに計算したMSE\n",
    "MSEs[0] = MSE1\n",
    "MSEs[1] = MSE2\n",
    "MSEs[2] = MSE3\n",
    "\n",
    "# 線形回帰モデルの設定\n",
    "regR = linear_model.LinearRegression()\n",
    "\n",
    "# 回帰変数用の配列の準備\n",
    "XX = np.copy(X3)\n",
    "for cols in range(cols_max):\n",
    "    # 一様乱数を振る\n",
    "    R = np.random.rand(Y.size, 1) # (150 x 1)の行列として設定する必要がある\n",
    "    \n",
    "    # 説明変数を新しい列として追加する\n",
    "    XX = np.append(XX, R, axis=1) # 列を追加するのには axis=1\n",
    "    \n",
    "    # 学習と予測をまとめてできる\n",
    "    YRp = regR.fit(XX, Y).predict(XX)\n",
    "    \n",
    "    # MSEの値を格納\n",
    "    MSEs[cols+3] = mean_squared_error(Y, YRp)\n",
    "    \n",
    "    # MSEを表示\n",
    "    print('説明変数の数 %3d: MSE = %5.3e    R^2 = %f'%(cols+4, mean_squared_error(Y, YRp), r2_score(Y, YRp)))\n",
    "    R2 = 1- np.mean((Y-YRp)**2) / np.var(Y)\n",
    "    R2a = ( (Y.size-1)*R2 - (cols+3) ) / (Y.size-(cols+3)-1)\n",
    "    print(R2, R2a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE のプロット\n",
    "# 横軸は回帰変数の数，縦軸は回帰誤差の平均二乗誤差\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel(\"number of explanatory variables\", size=24)\n",
    "ax.set_ylabel(\"MSE\", size=24)\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.plot(np.arange(1, cols_max+4), MSEs, linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE の対数プロット\n",
    "# 横軸は回帰変数の数，縦軸は回帰誤差の平均二乗誤差\n",
    "fig2 = plt.figure(figsize=(12, 8))\n",
    "ax2 = plt.axes()\n",
    "ax2.set_xlabel(\"number of explanatory variables\", size=24)\n",
    "ax2.set_ylabel(\"MSE\", size=24)\n",
    "ax2.tick_params(labelsize=20)\n",
    "ax2.plot(np.arange(1, cols_max+4), MSEs, linewidth=3)\n",
    "ax2.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSEだけを減らそうと思ったら，全く意味のない変数を足しても構わないのである\n",
    "\n",
    "実際に学習モデルの出力を確かめてみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(12, 8))\n",
    "ax3 = plt.axes()\n",
    "ax3.set_xlabel('SL', size=24)\n",
    "ax3.set_ylabel('PW', size=24)\n",
    "ax3.tick_params(labelsize=20)\n",
    "ax3.scatter(XX[:, 0], Y, s=200, label='true value')\n",
    "ax3.scatter(XX[:, 0], YRp, s=50, label='prediction')\n",
    "ax3.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "気持ち悪いくらいに完璧に学習モデルの出力が真値と合っている．\n",
    "\n",
    "---\n",
    "\n",
    "### 汎化誤差も見てみよう\n",
    "\n",
    "学習に用いたデータの出力が真値と合っていることは，学習がきちんと済んだことを示しているだけである．  \n",
    "統計モデルの確認には学習に用いていないデータに対しても出力が正しく合うかどうかを調べなければならない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSEを格納するための配列を準備\n",
    "MSEsTest = np.zeros(cols_max+3)\n",
    "\n",
    "# 目的変数の設定\n",
    "Ytrain = np.append(iris.data[0::3, 3], iris.data[1::3, 3])\n",
    "Ytest  = iris.data[2::3, 3]\n",
    "\n",
    "\n",
    "### 説明変数：SL ###\n",
    "Xtrain = np.append(iris.data[0::3, 0:1], iris.data[1::3, 0:1], axis=0)\n",
    "Xtest  = iris.data[2::3, 0:1]\n",
    "\n",
    "# 学習でデータでフィットして，テストデータで予測\n",
    "Ypred = regR.fit(Xtrain, Ytrain).predict(Xtest)\n",
    "\n",
    "# 汎化誤差\n",
    "MSEsTest[0] = mean_squared_error(Ytest, Ypred)\n",
    "print('説明変数の数 %3d: MSE = %f'%(1, MSEs[0]))\n",
    "\n",
    "\n",
    "### 説明変数：SLとSW ###\n",
    "Xtrain = np.append(iris.data[0::3, 0:2], iris.data[1::3, 0:2], axis=0)\n",
    "Xtest  = iris.data[2::3, 0:2]\n",
    "\n",
    "# 学習でデータでフィットして，テストデータで予測\n",
    "Ypred = regR.fit(Xtrain, Ytrain).predict(Xtest)\n",
    "\n",
    "# 汎化誤差\n",
    "MSEsTest[1] = mean_squared_error(Ytest, Ypred)\n",
    "print('説明変数の数 %3d: MSE = %f'%(2, MSEs[1]))\n",
    "\n",
    "\n",
    "### 説明変数：SLとSWとPL ###\n",
    "Xtrain = np.append(iris.data[0::3, 0:3], iris.data[1::3, 0:3], axis=0)\n",
    "Xtest  = iris.data[2::3, 0:3]\n",
    "\n",
    "# 学習でデータでフィットして，テストデータで予測\n",
    "Ypred = regR.fit(Xtrain, Ytrain).predict(Xtest)\n",
    "\n",
    "# 汎化誤差\n",
    "MSEsTest[2] = mean_squared_error(Ytest, Ypred)\n",
    "print('説明変数の数 %3d: MSE = %f'%(3, MSEs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in range(cols_max):\n",
    "    # 説明変数に一様乱数の列を追加\n",
    "    R = np.random.rand(Ytrain.size, 1)\n",
    "    Xtrain = np.append(Xtrain, R, axis=1)\n",
    "    \n",
    "    R = np.random.rand(Ytest.size, 1)\n",
    "    Xtest = np.append(Xtest, R, axis=1)\n",
    "    \n",
    "    # 学習でデータでフィットして，テストデータで予測\n",
    "    Ypred = regR.fit(Xtrain, Ytrain).predict(Xtest)\n",
    "    \n",
    "    # 汎化誤差\n",
    "    MSEsTest[cols+3] = mean_squared_error(Ytest, Ypred)\n",
    "    print('説明変数の数 %3d: MSE = %f'%(cols+4, MSEs[cols+3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE のプロット\n",
    "# 横軸は回帰変数の数，縦軸は回帰誤差の平均二乗誤差\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel(\"number of explanatory variables\", size=24)\n",
    "ax.set_ylabel(\"MSE\", size=24)\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.plot(np.arange(1, cols_max+4), MSEs, linewidth=3, label='training error')\n",
    "ax.plot(np.arange(1, cols_max+4), MSEsTest, linewidth=3, label='test error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE の対数プロット\n",
    "# 横軸は回帰変数の数，縦軸は回帰誤差の平均二乗誤差\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel(\"number of explanatory variables\", size=24)\n",
    "ax.set_ylabel(\"MSE\", size=24)\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.plot(np.arange(1, cols_max+4), MSEs, linewidth=3, label='training error')\n",
    "ax.plot(np.arange(1, cols_max+4), MSEsTest, linewidth=3, label='test error')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('学習誤差が最小となる回帰変数の数 = %d'%(np.argmin(MSEs)+4))\n",
    "print('汎化誤算が最小となる回帰変数の数 = %d'%(np.argmin(MSEsTest)+4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習誤差は乱数を付け足してもほぼ 0 まで下げることができる．\n",
    "一方．汎化誤差は下がらない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = plt.figure(figsize=(12, 8))\n",
    "ax4 = plt.axes()\n",
    "ax4.set_xlabel('SL', size=24)\n",
    "ax4.set_ylabel('PW', size=24)\n",
    "ax4.tick_params(labelsize=20)\n",
    "ax4.scatter(Xtest[:, 0], Ytest, s=200, label='true value')\n",
    "ax4.scatter(Xtest[:, 0], Ypred, s=50, label='prediction')\n",
    "ax4.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "Ypred1 = reg.fit(Xtrain[:, 0:3], Ytrain).predict(Xtest[:, 0:3])\n",
    "\n",
    "print('%3d変数のモデルの汎化誤差 = %f'%(cols_max+4, mean_squared_error(Ytest, Ypred)))\n",
    "print('SL, SW, PLを変数とするモデルの汎化誤差 = %f'%mean_squared_error(Ytest, Ypred1))\n",
    "\n",
    "fig4 = plt.figure(figsize=(12, 8))\n",
    "ax4 = plt.axes()\n",
    "ax4.set_xlabel('SL', size=24)\n",
    "ax4.set_ylabel('PW', size=24)\n",
    "ax4.tick_params(labelsize=20)\n",
    "ax4.scatter(Xtest[:, 0], Ytest, s=200, label='true value')\n",
    "ax4.scatter(Xtest[:, 0], Ypred1, s=50, label='prediction')\n",
    "ax4.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "汎化誤差は SL, SW, PL の3変数の線形モデルと大して差はない  \n",
    "学習誤差だけに着目するのは統計モデルを構築する上で適切でないことがわかる\n",
    "\n",
    "尚，決定係数\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^N (y_i - \\hat y_i)^2}{\\sum_{i=1}^N (y_i - \\bar y)^2} = 1 - \\frac{\\mathrm{MSE}}{\\mathrm{Var}(y)}\n",
    "$$\n",
    "\n",
    "は MSE が小さければ 1 に近づく．ここで，$y_i$ は $i$番目の目的変数，$\\hat y_i$ は $i$番目のデータの学習モデルによる予測値，$\\hat y$, $\\mathrm{Var}(y)$ は目的変数 $\\{y_i\\}_{i=1}^N$ の平均値と標本分散である．\n",
    "\n",
    "したがって学習データに対してのフィッティングを決定係数を用いて評価するのは危険である．\n",
    "\n",
    "ここで，平方和の計算の時にデータ数で割らずにそれぞれの平方和の自由度で割る評価もある．\n",
    "\n",
    "$$\n",
    "\\tilde R^2 = 1 - \\frac{\\frac1{N-p-1} \\sum_{i=1}^N (y_i - \\hat y_i)^2}{\\frac1{N-1} \\sum_{i=1}^N (y_i - \\bar y)^2} = 1 - \\frac{\\frac{N-1}{N-p-1}\\mathrm{MSE}}{\\tilde{\\mathrm{Var}}(y)}\n",
    "$$\n",
    "\n",
    "この $\\tilde R^2$ を自由度調整済み決定係数と呼ぶ．\n",
    "$p$ は回帰変数の数，$\\tilde{\\mathrm{Var}}(y)$ は目的変数 $\\{y_i\\}_{i=1}^N$ の不偏分散である．\n",
    "\n",
    "ここで，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde R^2 & = 1 - \\frac{N-1}{N-p-1} \\frac{\\sum_{i=1}^N (y_i - \\hat y_i)^2}{\\sum_{i=1}^N (y_i - \\bar y)^2} = \\frac{N-1}{N-p-1} \\left( \\frac{N-p-1}{N-1} -  \\frac{\\sum_{i=1}^N (y_i - \\hat y_i)^2}{\\sum_{i=1}^N (y_i - \\bar y)^2} \\right) \n",
    "\\\\\n",
    "& = \\frac{N-1}{N-p-1} \\left( 1 -  \\frac{\\sum_{i=1}^N (y_i - \\hat y_i)^2}{\\sum_{i=1}^N (y_i - \\bar y)^2}\\right) - \\frac{p}{N-p-1} = \\frac{(N-1) R^2 - p}{N-p-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "となるので，$\\tilde R^2 \\le 1$ である（負の値も取り得る）．\n",
    "\n",
    "上の定義による決定係数 $R^2$ は，定数項が含まれる回帰モデルでは非負の値を取るが，原点を通過する回帰モデルでは負の値を取り得ることに注意する．偏差平方和の分解が成立しないためである（各自で確かめてみよう）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = np.zeros(cols_max+3)\n",
    "R2a = np.zeros(cols_max+3)\n",
    "for cols in range(cols_max+3):\n",
    "    # 学習でデータでフィットして，テストデータで予測\n",
    "    Ypred = regR.fit(Xtrain[:, :cols+1], Ytrain).predict(Xtest[:, :cols+1])\n",
    "    \n",
    "    # 汎化誤差\n",
    "    #MSEsTest[cols+3] = mean_squared_error(Ytest, Ypred)\n",
    "    \n",
    "    print('説明変数の数 %3d: R^2 = %f'%(cols+4, r2_score(Ytest, Ypred)))    \n",
    "    R2 = 1- np.mean((Ytest-Ypred)**2) / np.var(Ytest)\n",
    "    R2a = ( (Ytest.size-1)*R2 - (cols+1) ) / (Ytest.size-(cols+1)-1)\n",
    "    print(R2, R2a)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn に入っている別のデータセット：糖尿病患者の診療データはなかなかフィッティングが難しいデータセットである．  \n",
    "これに対して，多項式回帰でも重回帰でもよいのでとにかくフィッティングして見せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Irisデータセット読み込み\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# データの特徴量\n",
    "print(diabetes.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- age: 年齢\n",
    "- sex: 性別\n",
    "- bmi: BMI = 体重kg / (身長m)^2\n",
    "- bp: 平均血圧\n",
    "- s1: 血清測定値1 (tc)\n",
    "- s2: 血清測定値2 (ldl)\n",
    "- s3: 血清測定値3 (hdl)\n",
    "- s4: 血清測定値4 (tch)\n",
    "- s5: 血清測定値5 (ltg)\n",
    "- s6: 血清測定値6 (glu)\n",
    "\n",
    "目標変数は1年後の疾患の進行状況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　データセット読み込み\n",
    "Y = diabetes.target\n",
    "X = diabetes.data[:, 2:3] # たとえば bmi\n",
    "\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div style=\"text-align: right;\">以上</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
