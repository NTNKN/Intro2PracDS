{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実践データ科学入門 2020年度木曜4限\n",
    "\n",
    "# 第4回 その2 正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook # if necessary to rotate figures in 3D plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import art3d\n",
    "from ipywidgets import interact\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正則化とは\n",
    "\n",
    "多項式回帰において，多項式の次数を十分上げると学習誤差を限りなく下げられることを見た．しかしこの場合は汎化誤差が大きくなる過学習を引き起こす可能性があるのであった．過学習を抑えるためには汎化誤差が上がらないような次数をテスト用のデータを用いて検証すれば良いが，学習時に過学習させないように工夫することも重要である．その一つが__正則化__と呼ばれる操作である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小二乗法による学習では\n",
    "\n",
    "$$\n",
    "E(a_0, a_1, \\ldots, a_N) := \\frac1N \\sum_{i=1}^N (e^{(i)})^2 = \\frac1N \\sum_{i=1}^N \\left( y^{(i)} - a_0 - \\sum_{j=1}^M a_j x_j^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "という平均二乗誤差を最小にするパラメータ $a_0, a_1, \\ldots, a_M$ を求めたのであった．\n",
    "\n",
    "正則化最小二乗法とは，ハイパーパラメータ $\\eta>0$ に対して\n",
    "\n",
    "$$\n",
    "\\tilde E(a_0, a_1, \\ldots, a_N; \\eta) := \\frac1N \\sum_{i=1}^N (e^{(i)})^2 + \\eta \\sum_{j=0}^M (a_j)^2\n",
    "= \\frac1N \\sum_{i=1}^N \\left( y^{(i)} - a_0 - \\sum_{j=1}^M a_j x_j^{(i)} \\right)^2 + \\eta \\sum_{j=0}^M (a_j)^2\n",
    "\\qquad ...(*)\n",
    "$$\n",
    "\n",
    "を最小にする $a_0, a_1, \\ldots, a_M$ を求める手法である．$(*)$ の右辺第二項を正則化項と呼ぶ．\n",
    "\n",
    "一般に過学習が行われるときは，過剰な回帰変数 $x_j$ にまで非零のパラメータ $a_j$ が与えられてしまい，結果的に\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^M (a_j)^2\n",
    "$$\n",
    "\n",
    "の値が大きくなっている．誤差を最小化させたくても $\\sum_{j=0}^M (a_j)^2$ が大きくなってしまうなら，それは最小化問題としては罰則として働いてしまう．そのため，$(*)$ の右辺第二項のことを罰則項とも呼ぶ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真のパラメータ\n",
    "A0 = 1.2\n",
    "A1 = 5.6\n",
    "A2 = 0.0\n",
    "\n",
    "# training data\n",
    "X1 = np.arange(-2, 1, 0.1)\n",
    "Y1 = A0 + A1*X1 + A2*X1**2 + 2.0*np.random.randn(X1.size)\n",
    "\n",
    "# test data\n",
    "X2 = np.arange(-2, 1, 0.1)\n",
    "Y2 = A0 + A1*X2 + A2*X2**2 + 2.0*np.random.randn(X2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回帰直線のプロット\n",
    "def plot_XY_and_regressionline(a0=0.0, a1=1.0):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel(\"X\", size=20)\n",
    "    ax.set_ylabel(\"Y\", size=20)\n",
    "    ax.set_xticks\n",
    "    #ax.set_ylim(-3, 3)\n",
    "    ax.scatter(X1, Y1)\n",
    "    ax.plot([np.min(X1), np.max(X1)], [a0+a1*np.min(X1), a0+a1*np.max(X1)], linewidth=3, color='tab:red')\n",
    "    ax.set_title('MSE = %f'%(np.sum((Y1-a0-a1*X1)**2)), size=20)\n",
    "    ax.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_XY_and_regressionline, a0=(-5.0, 5.0, 0.1), a1=(-5.0, 10.0, 0.1))\n",
    "\n",
    "# 青点がデータ点\n",
    "# 赤が回帰直線\n",
    "# MSE = Mean Squared Error = 平均二乗誤差\n",
    "\n",
    "# 必ずしも真のパラメータのときに平均二乗誤差最小になるわけではないことに注意しよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形モデルに対して強めのノイズを付加すると多項式フィッティングを用いたくなってしまう．この場合どうなるかみてみよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多項式フィッティング（復習）\n",
    "\n",
    "回帰変数2つの重回帰では，データ空間 $(x_1, x_2, y)$ の3次元空間に回帰平面を引き，データから平面までの高さの二乗和が最小になるように選ぶ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フィッティングに用いる多項式の最大次数を１から1つずつ増やしていく\n",
    "maxM = 20\n",
    "\n",
    "# dataset\n",
    "Xtrain = np.zeros((X1.size, 0))\n",
    "Xtest = np.zeros((X2.size, 0))\n",
    "\n",
    "# 回帰モデルの設定\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "MSE1 = np.zeros(maxM+1)\n",
    "MSE2 = np.zeros(maxM+1)\n",
    "anorm2 = np.zeros(maxM+1)\n",
    "coeff = np.zeros((maxM+1, maxM+1))\n",
    "\n",
    "for M in range(1, maxM+1):\n",
    "    Xtrain = np.append(Xtrain, (X1**M).reshape(-1, 1), axis=1)\n",
    "    Xtest = np.append(Xtest, (X2**M).reshape(-1, 1), axis=1)\n",
    "    Y1pred = reg.fit(Xtrain, Y1).predict(Xtrain)\n",
    "    Y2pred = reg.predict(Xtest)\n",
    "    \n",
    "    coeff[M, 0] = reg.intercept_\n",
    "    coeff[M, 1:M+1] = reg.coef_\n",
    "    MSE1[M] = mean_squared_error(Y1, Y1pred)\n",
    "    MSE2[M] = mean_squared_error(Y2, Y2pred)\n",
    "    anorm2[M] = np.sum( reg.coef_**2 ) + reg.intercept_**2\n",
    "\n",
    "fig1 = plt.figure(figsize=(16, 6))\n",
    "ax11 = fig1.add_subplot(121)\n",
    "ax11.plot(np.arange(1, maxM+1, 1), MSE1[1:], 'o-', linewidth=3, c='tab:blue', label='training MSE')\n",
    "ax11.plot(np.arange(1, maxM+1, 1), MSE2[1:], 'o-', linewidth=3, c='tab:orange', label='test MSE')\n",
    "ax11.tick_params(labelsize=14)\n",
    "ax11.set_xlabel(\"number of regression variables\", fontsize=18 )\n",
    "ax11.legend(fontsize=18)\n",
    "\n",
    "ax12 = fig1.add_subplot(122)\n",
    "ax12.set_yscale('log')\n",
    "ax12.plot(np.arange(1, maxM+1, 1), anorm2[1:], 'o-', linewidth=3, c='tab:blue', label='$||a||^2$')\n",
    "ax12.tick_params(labelsize=14)\n",
    "ax12.set_xlabel(\"number of regression variables\", fontsize=18 )\n",
    "ax12.legend(fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多項式回帰関数のプロット\n",
    "def plot_XY_and_PolyReg(M=1):\n",
    "    XX = np.arange(np.min(X1), np.max(X1), 0.01)\n",
    "    YY = coeff[M, 0]\n",
    "    for i in range(1, M+1):\n",
    "        YY += coeff[M, i] * XX**i\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.set_xlabel(\"X\", size=20)\n",
    "    ax1.set_ylabel(\"Y\", size=20)\n",
    "    ax1.set_xticks\n",
    "    ax1.set_ylim(np.minimum(np.min(Y1), np.min(Y2)), np.maximum(np.max(Y1), np.max(Y2)))\n",
    "    ax1.plot(XX, YY, linewidth=3)\n",
    "    ax1.scatter(X1, Y1, s=100)\n",
    "    ax1.tick_params(labelsize=12)\n",
    "    ax1.text(-2, 7, 'MSE = %f'%(MSE1[M]), fontsize=20)\n",
    "    \n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.set_xlabel(\"X\", size=20)\n",
    "    ax2.set_ylabel(\"Y\", size=20)\n",
    "    ax2.set_xticks\n",
    "    ax2.set_ylim(np.minimum(np.min(Y1), np.min(Y2)), np.maximum(np.max(Y1), np.max(Y2)))\n",
    "    ax2.plot(XX, YY, linewidth=3)\n",
    "    ax2.scatter(X2, Y2, c='tab:orange', s=100)\n",
    "    ax2.tick_params(labelsize=12)\n",
    "    ax2.text(-2, 7, 'MSE = %f'%(MSE2[M]), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_XY_and_PolyReg, M=(1, maxM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正則化パラメータ\n",
    "alpha = 0.1\n",
    "\n",
    "# フィッティングに用いる多項式の最大次数を１から1つずつ増やしていく\n",
    "maxM = 20\n",
    "\n",
    "# dataset\n",
    "Xtrain = np.zeros((X1.size, 0))\n",
    "Xtest = np.zeros((X2.size, 0))\n",
    "\n",
    "# 回帰モデルの設定\n",
    "ridge = linear_model.Ridge(alpha=alpha)\n",
    "\n",
    "MSE1 = np.zeros(maxM+1)\n",
    "MSE2 = np.zeros(maxM+1)\n",
    "anorm2 = np.zeros(maxM+1)\n",
    "coeff = np.zeros((maxM+1, maxM+1))\n",
    "\n",
    "for M in range(1, maxM+1):\n",
    "    Xtrain = np.append(Xtrain, (X1**M).reshape(-1, 1), axis=1)\n",
    "    Xtest = np.append(Xtest, (X2**M).reshape(-1, 1), axis=1)\n",
    "    Y1pred = ridge.fit(Xtrain, Y1).predict(Xtrain)\n",
    "    Y2pred = ridge.predict(Xtest)\n",
    "    \n",
    "    coeff[M, 0] = ridge.intercept_\n",
    "    coeff[M, 1:M+1] = ridge.coef_\n",
    "    MSE1[M] = mean_squared_error(Y1, Y1pred)\n",
    "    MSE2[M] = mean_squared_error(Y2, Y2pred)\n",
    "    anorm2[M] = np.sum( ridge.coef_**2 ) + ridge.intercept_**2\n",
    "\n",
    "fig1 = plt.figure(figsize=(16, 6))\n",
    "ax11 = fig1.add_subplot(121)\n",
    "ax11.plot(np.arange(1, maxM+1, 1), MSE1[1:], 'o-', linewidth=3, c='tab:blue', label='training MSE')\n",
    "ax11.plot(np.arange(1, maxM+1, 1), MSE2[1:], 'o-', linewidth=3, c='tab:orange', label='test MSE')\n",
    "ax11.tick_params(labelsize=14)\n",
    "ax11.set_xlabel(\"number of regression variables\", fontsize=18 )\n",
    "ax11.legend(fontsize=18)\n",
    "\n",
    "ax12 = fig1.add_subplot(122)\n",
    "ax12.set_yscale('log')\n",
    "ax12.plot(np.arange(1, maxM+1, 1), anorm2[1:], 'o-', linewidth=3, c='tab:blue', label='$||a||^2$')\n",
    "ax12.tick_params(labelsize=14)\n",
    "ax12.set_xlabel(\"number of regression variables\", fontsize=18 )\n",
    "ax12.legend(fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多項式回帰関数のプロット\n",
    "def plot_XY_and_PolyRegRidge(M=1):\n",
    "    XX = np.arange(np.min(X1), np.max(X1), 0.01)\n",
    "    YY = coeff[M, 0]\n",
    "    for i in range(1, M+1):\n",
    "        YY += coeff[M, i] * XX**i\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.set_xlabel(\"X\", size=20)\n",
    "    ax1.set_ylabel(\"Y\", size=20)\n",
    "    ax1.set_xticks\n",
    "    ax1.set_ylim(np.minimum(np.min(Y1), np.min(Y2)), np.maximum(np.max(Y1), np.max(Y2)))\n",
    "    ax1.plot(XX, YY, linewidth=3)\n",
    "    ax1.scatter(X1, Y1, s=100)\n",
    "    ax1.tick_params(labelsize=12)\n",
    "    ax1.text(-2, 7, 'MSE = %f'%(MSE1[M]), fontsize=20)\n",
    "    ax1.text(-2, 5, 'AIC = %f'%(-2*np.log(MSE1[M])+2*M), fontsize=20)    \n",
    "    \n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.set_xlabel(\"X\", size=20)\n",
    "    ax2.set_ylabel(\"Y\", size=20)\n",
    "    ax2.set_xticks\n",
    "    ax2.set_ylim(np.minimum(np.min(Y1), np.min(Y2)), np.maximum(np.max(Y1), np.max(Y2)))\n",
    "    ax2.plot(XX, YY, linewidth=3)\n",
    "    ax2.scatter(X2, Y2, c='tab:orange', s=100)\n",
    "    ax2.tick_params(labelsize=12)\n",
    "    ax2.text(-2, 7, 'MSE = %f'%(MSE2[M]), fontsize=20)\n",
    "    ax2.text(-2, 5, 'AIC = %f'%(-2*np.log(MSE2[M])+2*M), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_XY_and_PolyRegRidge, M=(1, maxM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris データセットに対して，Ridge 正則化によるフィッティングと AIC によるモデル選択（正則化は行わない）ではどのような差が起きるか． \n",
    "データを解析して両者を比較し，考察せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div style=\"text-align: right;\">以上</div></h3>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
